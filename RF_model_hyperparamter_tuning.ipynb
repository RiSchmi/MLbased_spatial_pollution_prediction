{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross evaluate the performance of each predictor individually based on its R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing for Random Forest\n",
    "\n",
    "0. load and sort data frame (temporal dependency is overcome)\n",
    "\n",
    "1. remove instances with imputed target variables, since temporal consistency is not needed for random forest & missing data type is completely at random --> no introduced bias\n",
    "\n",
    "2. add column of weighted_mean_pollution: for each timestamp (distance based) mean of pollution from other stations \n",
    "\n",
    "3. separate 10 percent of instances of each group (id) for test propose, rest for training and validation\n",
    "\n",
    "4. incremental feature section based on prior MI and VIF based feature selection\n",
    "\n",
    "5. hyperparamter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'MESS_DATUM', 'id', 'Stickstoffdioxid', 'prec_mm',\n",
       "       'prec_bool', 'humidity', 'temp', 'radiation', 'wind_degree',\n",
       "       'wind_speed', 'air_pressure', 'free_wind', 'prop_intercept_200',\n",
       "       'prop_intercept_50', 'GVI_25', 'GVI_50', 'GVI_75', 'GVI_100', 'GVI_200',\n",
       "       'tvi_25', 'tvi_50', 'tvi_75', 'tvi_100', 'tvi_200', 'prop_main_',\n",
       "       'nearest_st', 'nearest_in', 'pop_200', 'pop_500', 'weekend', 'rushhour',\n",
       "       'lai_factor', 'distance_city'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. load (and sort) data frame\n",
    "seed = 42\n",
    "main = pd.read_csv('data/df_processed.csv').sample(frac=1, random_state= seed)\n",
    "main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed Instances excluded: 722\n"
     ]
    }
   ],
   "source": [
    "# 1. remove instances with imputed target variable\n",
    "import pickle\n",
    "\n",
    "# load indexes of imputed values\n",
    "with open('data/timesteps_missing_data.pkl', 'rb') as pickle_file:\n",
    "    timestep_missing_data = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "# exclude values \n",
    "main_no_impute = main.copy()\n",
    "\n",
    "for id in timestep_missing_data.keys():  \n",
    "    for timestep in timestep_missing_data.get(id):\n",
    "        #exclude row which match indexing (id and timestep of imputed value)\n",
    "        main_no_impute = main_no_impute[~((main_no_impute['id'] == id) & (main_no_impute['time_step'] == timestep))]\n",
    "\n",
    "print(f'Imputed Instances excluded: {main.shape[0] - main_no_impute.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. add column of weighted mean pollution\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate weighted mean pollution based on similarity in distance to the city center\n",
    "def weighted_mean_pollution(row):\n",
    "\n",
    "    # Filter for other stations at the same timestamp\n",
    "    same_time = main[main['time_step'] == row['time_step']]\n",
    "    \n",
    "    # Exclude the current station\n",
    "    other_stations = same_time[same_time['id'] != row['id']]\n",
    "    \n",
    "    # similarity weights based on the inverse of the absolute difference in distances\n",
    "    weights = 1 / (1 + np.abs(other_stations['distance_city'] - row['distance_city']))\n",
    "       \n",
    "    # Calculate the weighted mean pollution\n",
    "    weighted_mean = np.average(other_stations['NO2'], weights=weights)\n",
    "\n",
    "    return weighted_mean\n",
    "\n",
    "\n",
    "# add weighted mean pollution for each time step\n",
    "main_no_impute['weighted_mean_pollution'] = main_no_impute.apply(weighted_mean_pollution, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\AppData\\Local\\Temp\\ipykernel_21448\\73997058.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['weighted_mean_pollution'] = train_df.apply(weighted_mean_pollution, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# 3. separate test from training station based on id \n",
    "# TEST STATIONS: mc117, mc018, mc145\n",
    "main = main_no_impute.copy()\n",
    "test_data_mc117 = main[main['id'] == 'mc117'].sort_values('time_step')\n",
    "test_data_mc018 = main[main['id'] == 'mc018'].sort_values('time_step')\n",
    "test_data_mc145 = main[main['id'] == 'mc145'].sort_values('time_step')\n",
    "\n",
    "train_df = main[~main['id'].isin(['mc117', 'mc018', 'mc145'])]\n",
    "train_df['weighted_mean_pollution'] = train_df.apply(weighted_mean_pollution, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 create 18 look up tables for weighted pollution \n",
    "def construct_LOOCV_weighted_pollution(df):\n",
    "    for site in list(df['id'].unique()):\n",
    "        current_df = df[df['id'] != site][['time_step', 'id', 'NO2','distance_city']]\n",
    "        current_df['weighted_pollution_df'] = current_df.apply(weighted_mean_pollution, axis=1)\n",
    "        current_df = current_df[['time_step', 'id', 'weighted_pollution_df']]\n",
    "        current_df.to_csv(f'data/pollution_data/LOOCV/weighted_pollution_no_{site}.csv')\n",
    "\n",
    "construct_LOOCV_weighted_pollution(df = train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate feature importance through model diagnostics with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['prec_mm', 'prec_bool', 'humidity', 'temp', 'radiation', 'wind_degree',\n",
      "       'wind_speed', 'air_pressure', 'free_wind', 'prop_intercept_200',\n",
      "       'prop_intercept_50', 'GVI_25', 'GVI_50', 'GVI_75', 'GVI_100', 'GVI_200',\n",
      "       'tvi_25', 'tvi_50', 'tvi_75', 'tvi_100', 'tvi_200', 'prop_main_',\n",
      "       'nearest_st', 'nearest_in', 'pop_200', 'pop_500', 'weekend', 'rushhour',\n",
      "       'lai_factor', 'weighted_mean_pollution'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mc221</th>\n",
       "      <td>0.743825</td>\n",
       "      <td>6.812780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc115</th>\n",
       "      <td>0.077539</td>\n",
       "      <td>10.560281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc282</th>\n",
       "      <td>0.636920</td>\n",
       "      <td>5.496195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc190</th>\n",
       "      <td>0.748814</td>\n",
       "      <td>7.333007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc042</th>\n",
       "      <td>0.824352</td>\n",
       "      <td>4.581197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc174</th>\n",
       "      <td>0.604628</td>\n",
       "      <td>8.326037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc077</th>\n",
       "      <td>0.531797</td>\n",
       "      <td>4.668812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc010</th>\n",
       "      <td>0.716982</td>\n",
       "      <td>5.906101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc027</th>\n",
       "      <td>0.545918</td>\n",
       "      <td>5.069790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc032</th>\n",
       "      <td>0.489451</td>\n",
       "      <td>4.908707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc171</th>\n",
       "      <td>0.811298</td>\n",
       "      <td>4.836624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc124</th>\n",
       "      <td>0.445600</td>\n",
       "      <td>10.735812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mc143</th>\n",
       "      <td>0.494813</td>\n",
       "      <td>12.244285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.590149</td>\n",
       "      <td>7.036894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             R2        MAE\n",
       "mc221  0.743825   6.812780\n",
       "mc115  0.077539  10.560281\n",
       "mc282  0.636920   5.496195\n",
       "mc190  0.748814   7.333007\n",
       "mc042  0.824352   4.581197\n",
       "mc174  0.604628   8.326037\n",
       "mc077  0.531797   4.668812\n",
       "mc010  0.716982   5.906101\n",
       "mc027  0.545918   5.069790\n",
       "mc032  0.489451   4.908707\n",
       "mc171  0.811298   4.836624\n",
       "mc124  0.445600  10.735812\n",
       "mc143  0.494813  12.244285\n",
       "total  0.590149   7.036894"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple cross- validation for  feature importance test\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def site_wise_RF(df):\n",
    "\n",
    "    # store r2 and mae performance\n",
    "    performance = {}\n",
    "\n",
    "    # store feature wise performance\n",
    "    index_names = list(df.columns.drop(['Unnamed: 0', 'time_step', 'id', 'NO2']))\n",
    "    feature_performance = pd.DataFrame(index=index_names)\n",
    "    main = df.copy()\n",
    "\n",
    "    n = 0 \n",
    "    # initiate iteration over sites\n",
    "\n",
    "\n",
    "    for site in list(main['id'].unique()):\n",
    "\n",
    "        # initialize test set\n",
    "        X_test = main[main['id'] == site].drop(['Unnamed: 0', 'time_step', 'id', 'NO2','distance_city'], axis = 1)\n",
    "        y_test = main[main['id'] == site]['NO2']\n",
    "\n",
    "        # initialize training set + weighted pollution \n",
    "        train = main[main['id'] != site].drop('weighted_mean_pollution', axis = 1)\n",
    "        \n",
    "        train['weighted_mean_pollution'] = train.apply(weighted_mean_pollution, axis=1)    \n",
    "\n",
    "        X_train = train.drop(['Unnamed: 0', 'time_step', 'id', 'NO2','distance_city'], axis = 1)\n",
    "        y_train = train['NO2']\n",
    "\n",
    "        # train model \n",
    "        model = RandomForestRegressor(n_jobs=-1) # initiate model\n",
    "        model.fit(X_train, y_train) # train model based on single feature\n",
    "           \n",
    "        # evaluate model on r2 and mse    \n",
    "        y_pred = model.predict(X_test) # evaluate model\n",
    "        r2  = r2_score(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "        performance[site] = [r2, mse] # store performance\n",
    "                \n",
    "        # feature importance\n",
    "        global_importances = pd.Series(100*model.feature_importances_, index=X_train.columns)\n",
    "        feature_performance = pd.concat([feature_performance, global_importances], axis=1)\n",
    "        \n",
    "        # return process \n",
    "        n += 1\n",
    "        if n%4 == 0:\n",
    "            print(f\"Process: {100 * (n / len(list(main['id'].unique())))}\")\n",
    "\n",
    "    print(X_train.columns)\n",
    "\n",
    "    return feature_performance, performance\n",
    "\n",
    "features = ['Unnamed: 0', 'time_step', 'id', 'NO2', 'free_wind', 'prop_intercept_200',\n",
    "'prop_intercept_50', 'GVI_25', 'GVI_50', 'GVI_75', 'GVI_100', 'GVI_200',\n",
    "'tvi_25', 'tvi_50', 'tvi_75', 'tvi_100', 'tvi_200', 'prop_main_',\n",
    "'nearest_st', 'nearest_in', 'pop_200', 'pop_500', 'weekend', 'rushhour',\n",
    "'lai_factor', 'distance_city', 'weighted_mean_pollution']\n",
    "\n",
    "#feature_performance, performance = site_wise_RF(df = train_df[ features])\n",
    "feature_performance, performance = site_wise_RF(df = train_df)\n",
    "\n",
    "feature_performance_ordered = list(feature_performance.index)\n",
    "\n",
    "# per site performance\n",
    "performance_pf = pd.DataFrame(performance)\n",
    "performance_pf['total'] = performance_pf.mean(axis=1)\n",
    "performance_pf = performance_pf.transpose().rename({0: 'R2', 1 : 'MAE' }, axis = 1)\n",
    "#performance_pf.to_csv('data/output/baseline_performance/R2_MAE_2.csv')\n",
    "performance_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple cross- validation for  feature importance test\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def site_wise_RF2(df, features,parameter = None):\n",
    "\n",
    "    # store r2 and mae performance\n",
    "    performance = {}\n",
    "\n",
    "    main = df.copy()\n",
    "\n",
    "    n = 0 \n",
    "    # initiate iteration over sites\n",
    "\n",
    "\n",
    "    for site in list(main['id'].unique()):\n",
    "\n",
    "        # initialize test set\n",
    "        X_test = main[main['id'] == site][features] .drop(['NO2'], axis = 1)\n",
    "        y_test = main[main['id'] == site]['NO2']\n",
    "\n",
    "        # initialize training set + weighted pollution \n",
    "        train = main[main['id'] != site].drop('weighted_mean_pollution', axis = 1)\n",
    "        train_weighted_pollution = pd.read_csv(f'data/pollution_data/LOOCV/weighted_pollution_no_{site}.csv')\n",
    "        train_weighted_pollution = train_weighted_pollution.rename({'weighted_pollution_df':'weighted_mean_pollution'}, axis=1)\n",
    "        train = pd.concat([train.reset_index(), train_weighted_pollution.reset_index()], axis= 1)\n",
    "        \n",
    "        X_train = train[features].drop(['NO2'], axis = 1)\n",
    "        y_train = train['NO2']\n",
    "\n",
    "        # train model \n",
    "        if parameter == None:\n",
    "            model = RandomForestRegressor(n_jobs=-1) # initiate model\n",
    "        else:\n",
    "            model = RandomForestRegressor(**parameter, n_jobs=-1) # initiate model with defined parameter\n",
    "        model.fit(X_train, y_train) # train model based on single feature\n",
    "           \n",
    "        # evaluate model on r2 and mse    \n",
    "        y_pred = model.predict(X_test) # evaluate model\n",
    "        performance[site] = [r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred, squared=False)] # store performance\n",
    "         \n",
    "        \n",
    "        # return process \n",
    "        n += 1\n",
    "        if n%4 == 0:\n",
    "            print(f\"Process: {100 * (n / len(list(main['id'].unique())))}\")\n",
    "\n",
    "    print(X_train.columns)\n",
    "\n",
    "    return performance\n",
    "\n",
    "features = ['Unnamed: 0', 'time_step', 'id', 'NO2', 'free_wind', 'prop_intercept_200',\n",
    "'prop_intercept_50', 'GVI_25', 'GVI_50', 'GVI_75', 'GVI_100', 'GVI_200',\n",
    "'tvi_25', 'tvi_50', 'tvi_75', 'tvi_100', 'tvi_200', 'prop_main_',\n",
    "'nearest_st', 'nearest_in', 'pop_200', 'pop_500', 'weekend', 'rushhour',\n",
    "'lai_factor', 'distance_city', 'weighted_mean_pollution']\n",
    "\n",
    "#feature_performance, performance = site_wise_RF(df = train_df[ features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['nearest_in', 'prop_intercept_200', 'pop_500', 'GVI_25', 'prop_main_',\n",
      "       'tvi_200', 'free_wind', 'lai_factor', 'wind_speed', 'temp',\n",
      "       'wind_degree', 'rushhour', 'weekend', 'prec_mm', 'prec_bool',\n",
      "       'weighted_mean_pollution'],\n",
      "      dtype='object')\n",
      "Performance R2: -0.8838482253569419, mse: 12.838350031962971 with 15 features.\n"
     ]
    }
   ],
   "source": [
    "# incremental feature selection\n",
    "def return_mean_r2(performance, metrics = 'R2'):\n",
    "    performance_pf = pd.DataFrame(performance)\n",
    "    performance_pf['total'] = performance_pf.mean(axis=1)\n",
    "    performance_pf = performance_pf.transpose().rename({0: 'R2', 1 : 'MAE' }, axis = 1)\n",
    "    return performance_pf.loc['total', metrics]\n",
    "    \n",
    "\n",
    "def feature_section(feature_performance_ordered):\n",
    "\n",
    "    current_features = feature_performance_ordered[1:15] # initialize start with 5 best performing features\n",
    "    remaining_features = feature_performance_ordered[15:] # define remaining features\n",
    "    \n",
    "    # iterate over ordered list of remaining features\n",
    "    for feature in remaining_features:\n",
    "        current_features.append(feature)\n",
    "\n",
    "        performance = site_wise_RF2(df = train_df, features= ['NO2'] + current_features + ['weighted_mean_pollution'])\n",
    "        \n",
    "        mean_performance = return_mean_r2(performance)\n",
    "                \n",
    "        print(f\"Performance R2: {mean_performance}, mse: {return_mean_r2(performance, metrics= 'MAE')} with {len(current_features)} features.\")\n",
    "\n",
    "    return performance, current_features\n",
    "\n",
    "#feature_performance_ordered = list(pd.read_csv('data/output/baseline_performance/feature_importance_1.csv')['Unnamed: 0'])\n",
    "feature_performance_ordered = ['weighted_mean_pollution','nearest_in', 'prop_intercept_200', 'pop_500','GVI_25',\n",
    "            'prop_main_','tvi_200', 'free_wind','lai_factor','wind_speed','temp','wind_degree',\n",
    "            'rushhour', 'weekend', 'prec_mm', 'prec_bool']\n",
    "\n",
    "\n",
    "performance, current_features = feature_section(feature_performance_ordered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'tvi_25', 'tvi_50', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
    "'air_pressure', 'lai_factor', 'nearest_st', 'radiation', 'rushhour',\n",
    "'pop_500', 'prop_main_', 'pop_200', 'prop_intercept_200', 'tvi_75',\n",
    "'GVI_200', 'distance_city', 'prop_intercept_50', 'nearest_in',\n",
    "'tvi_200', 'weighted_mean_pollution'\n",
    "\n",
    "\n",
    "Performance R2: 0.5864718786581627, mse: 7.011109526152897 with 21 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter Testing  \n",
    "\n",
    "1. run iteratively over all hyperparameter\n",
    "2. remain best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['tvi_25', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
      "       'air_pressure', 'lai_factor', 'pop_500', 'prop_main_',\n",
      "       'prop_intercept_200', 'GVI_200', 'prop_intercept_50', 'tvi_200',\n",
      "       'free_wind', 'tvi_50', 'nearest_st', 'weighted_mean_pollution'],\n",
      "      dtype='object')\n",
      "new best parameter: {'max_depth': None} with R2: 0.6057801203855591\n",
      "R2 of: 0.6057801203855591 with max_depth and {'max_depth': None}\n",
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['tvi_25', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
      "       'air_pressure', 'lai_factor', 'pop_500', 'prop_main_',\n",
      "       'prop_intercept_200', 'GVI_200', 'prop_intercept_50', 'tvi_200',\n",
      "       'free_wind', 'tvi_50', 'nearest_st', 'weighted_mean_pollution'],\n",
      "      dtype='object')\n",
      "new best parameter: {'max_depth': None, 'max_features': 'sqrt'} with R2: 0.6119440989804145\n",
      "R2 of: 0.6119440989804145 with max_features and {'max_depth': None, 'max_features': 'sqrt'}\n",
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['tvi_25', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
      "       'air_pressure', 'lai_factor', 'pop_500', 'prop_main_',\n",
      "       'prop_intercept_200', 'GVI_200', 'prop_intercept_50', 'tvi_200',\n",
      "       'free_wind', 'tvi_50', 'nearest_st', 'weighted_mean_pollution'],\n",
      "      dtype='object')\n",
      "R2 of: 0.6032994178616077 with min_samples_leaf and {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4}\n",
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['tvi_25', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
      "       'air_pressure', 'lai_factor', 'pop_500', 'prop_main_',\n",
      "       'prop_intercept_200', 'GVI_200', 'prop_intercept_50', 'tvi_200',\n",
      "       'free_wind', 'tvi_50', 'nearest_st', 'weighted_mean_pollution'],\n",
      "      dtype='object')\n",
      "R2 of: 0.6042289649309188 with min_samples_split and {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['tvi_25', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
      "       'air_pressure', 'lai_factor', 'pop_500', 'prop_main_',\n",
      "       'prop_intercept_200', 'GVI_200', 'prop_intercept_50', 'tvi_200',\n",
      "       'free_wind', 'tvi_50', 'nearest_st', 'weighted_mean_pollution'],\n",
      "      dtype='object')\n",
      "new best parameter: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'criterion': 'squared_error'} with R2: 0.6177836763254099\n",
      "R2 of: 0.6177836763254099 with criterion and {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'criterion': 'squared_error'}\n",
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['tvi_25', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
      "       'air_pressure', 'lai_factor', 'pop_500', 'prop_main_',\n",
      "       'prop_intercept_200', 'GVI_200', 'prop_intercept_50', 'tvi_200',\n",
      "       'free_wind', 'tvi_50', 'nearest_st', 'weighted_mean_pollution'],\n",
      "      dtype='object')\n",
      "R2 of: 0.6147364053354962 with criterion and {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'criterion': 'friedman_mse'}\n",
      "Process: 30.76923076923077\n",
      "Process: 61.53846153846154\n",
      "Process: 92.3076923076923\n",
      "Index(['tvi_25', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
      "       'air_pressure', 'lai_factor', 'pop_500', 'prop_main_',\n",
      "       'prop_intercept_200', 'GVI_200', 'prop_intercept_50', 'tvi_200',\n",
      "       'free_wind', 'tvi_50', 'nearest_st', 'weighted_mean_pollution'],\n",
      "      dtype='object')\n",
      "R2 of: 0.6059586175662411 with criterion and {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'criterion': 'poisson'}\n"
     ]
    }
   ],
   "source": [
    "def hyperparameter_search_random_forest(df, features, param_grid, best_performance):\n",
    "    '''\n",
    "    Iterate over parameter grid and rerun RF with parameters and store best performing per parameter\n",
    "    '''\n",
    "    best_performance = best_performance # adjust to previous best performance\n",
    "    best_parameter = {}\n",
    "    current_parameter = {}\n",
    "\n",
    "    for hyperparameter in param_grid.keys():\n",
    "        current_parameter = best_parameter\n",
    "        for parameter in list(param_grid.get(hyperparameter)):\n",
    "            current_parameter[hyperparameter] = parameter\n",
    "            \n",
    "            performance = site_wise_RF2(df = df,\n",
    "                                        features= features,\n",
    "                                        parameter = current_parameter)\n",
    "            \n",
    "            mean_performance = return_mean_r2(performance)        \n",
    "            \n",
    "            if mean_performance > best_performance:\n",
    "                best_performance = mean_performance\n",
    "                best_parameter[hyperparameter] = parameter\n",
    "                print(f'new best parameter: {best_parameter} with R2: {mean_performance}')\n",
    "            # run function with parameter & hyperparamter\n",
    "            print(f'R2 of: {mean_performance} with {hyperparameter} and {current_parameter}')\n",
    "\n",
    "\n",
    "# define parameter grid to search over\n",
    "param_grid = {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "              'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "              'max_features': [1.0, 'sqrt'],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000] }\n",
    "\n",
    "param_grid = {'max_depth': [None], 'max_features': ['sqrt'], 'min_samples_leaf': [4],\n",
    "              'min_samples_split': [5], 'criterion': ['squared_error', 'friedman_mse', 'poisson']}\n",
    "\n",
    "# define used features\n",
    "features= ['NO2', 'tvi_25', 'wind_degree', 'temp', 'humidity', 'wind_speed',\n",
    "       'air_pressure', 'lai_factor', 'pop_500', 'prop_main_',\n",
    "       'prop_intercept_200', 'GVI_200', 'prop_intercept_50', 'tvi_200',\n",
    "       'free_wind', 'tvi_50', 'nearest_st', 'weighted_mean_pollution']\n",
    "      \n",
    "hyperparameter_search_random_forest(df = train_df, features = features, param_grid = param_grid, best_performance = 0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
